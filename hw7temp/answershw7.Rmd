---
title: "HW7 Answers"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(dplyr); library(glmnet)
```

## Question 1

Use squared error loss (a linear model) and the variables waist, age, sex and bmi to predict log(rdi4p + 1). Interpet the coefficients.
```{r multivar}
dat = read.csv("shhs1.txt", header=T, sep="\t")
head(dat)
# using waist, age, sex, bmi to predict log(rdi4p + 1)
y = log(dat$rdi4p + 1)

summary(lm(y ~ waist + age_s1 + gender + bmi_s1, data = dat))
```
Intercept = log(rdi4p) is -2.28 if the person is female and has variables of waist, age, and BMI that are 0.

Waist coefficient = there is a 0.007 increase in log(rdi4p) for every one unit increase in waist holding remaining variables constant.

Age coefficient = there is a 0.02 increase in log(rdi4p) for every one year increase in age holding remaining variables constant.

Gender coefficient = there is a 0.518 increase in log(rdi4p) for males compared to females holding remaining variables constant.

BMI coefficient = there is a 0.063 increase in log(rdi4p) for every one unit increase in BMI.


## Question 2

Define sleep disordered breathing as having an rdi4p greater than 15. Use logistic regression to fit a model on having sleep disordered breathing using waist, age, sex and bmi as predictors. Interpret your coefficients.

```{r logit 15}
# check for NA values
#sapply(dat,function(x) sum(is.na(x)))

logDat = dat
logDat = logDat %>% mutate(sdb = case_when(rdi4p > 15 ~ 1, rdi4p <= 15 ~ 0, TRUE ~ NA_real_))

# waist, age, sex, bmi as predictors
fit = glm(sdb ~ waist + age_s1 + gender + bmi_s1, family = binomial(link = 'logit'), data = logDat)
summary(fit)

```
Intercept = -9.12 is the log odds when the all regressors are 0.

Waist coefficient = 0.019

Age coefficient = 0.033

Gender coefficient = 1.05

BMI coefficient = 0.103


## Question 3

Use a logistic regresion model to predict HTNDerv_S1 using age, sex, bmi and rdi4p. Interpret the rdi4p coefficient.

```{r logit HTNDerv}

fit3 = glm(HTNDerv_s1 ~ age_s1 + gender + bmi_s1 + rdi4p, family = binomial(link = 'logit'), data = dat)
summary(fit3)
```

rdi4p coefficient = 


## Question 4

Redo your fit to question 1 using a lasso penalty. Attach a plot of the coefficients as the L1 sum of the parameters varies.
```{r lasso}

# using waist, age, sex, bmi to predict log(rdi4p + 1)
penaltyDat = dat
penaltyDat = na.omit(penaltyDat)

x = penaltyDat %>% select(waist, age_s1, gender, bmi_s1) %>% as.matrix()
y = log(penaltyDat$rdi4p + 1)

## fit with our selected lambda
plot(glmnet(x, y, alpha = 1))

## pick the lasso parameter using cv
cvL = cv.glmnet(x, y, alpha = 1, 
                lambda = seq(0, exp(-3), length = 100))

plot(cvL)

newx  = x[1 : 5,]
predict(cvL, newx = newx, lambda = "lambda.1se")


```


## Question 5

Redo your fit to question 1 using a ridge peanlty. Attach a plot of the coefficients as the L1 sum of the parameters varies.
```{r ridge}

#predict(cvR, newx = x, lambda = "lambda.minR")
plot(glmnet(x, y, alpha = 0))


```

## Question 6

Redo your fit to question 1 using an elastic net penalty. Attach a plot of the coefficients as the L1 sum of the parameters varies.
```{r elastic }
plot(glmnet(x, y, alpha = 0.2))

```

## Question 7

Train your model from questions 1 to predict log(rdi4p + 1) using an elastic net, lasso, ridge regression, and no penalty. Use cross validation to select the penalty. Compare the best cross validated error for each type (lasso, ridge, EL).

```{r training}
plot(glmnet(x, y))

```



